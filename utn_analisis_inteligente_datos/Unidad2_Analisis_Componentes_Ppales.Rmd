---
title: "Analisis_componentes_Ppales"
author: "Área de Planificación Gestión y Estadística"
date: "21/3/2022"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F)
source("Unidad0_repos_and_tools.R")
# https://rpubs.com/ercalel/593452
# https://bookdown.org/jsalinas/tecnicas_multivariadas/acp.html
```

# Analisis componente principal

**Bibliografìa**: 

- https://rpubs.com/Cristina_Gil/PCA 
- https://rstudio-pubs-static.s3.amazonaws.com/471326_681054305d29404083097c8e501f9fbd.html#:~:text=%E2%80%9CLa%20idea%20central%20del%20an%C3%A1lisis,el%20conjunto%20de%20datos%E2%80%9D%20(Jolliffe
- Comprensión de Matrices Ejemplo libro AID, p.88

Una de las aplicaciones de PCA es la reducción de dimensionalidad (variables), perdiendo la menor cantidad de información (varianza) posible: cuando contamos con un gran número de variables cuantitativas posiblemente correlacionadas (indicativo de existencia de información redundante), PCA permite reducirlas a un número menor de variables transformadas (componentes principales) que expliquen gran parte de la variabilidad en los datos. Cada dimensión o componente principal generada por PCA será una combinación lineal de las variables originales, y serán además independientes o no correlacionadas entre sí.

El análisis de CP tiene su fundamento en la necesidad de analizar las relaciones que se establecen entre  múltiples variables cuantitativas. Por ejemplo, la correlación establece una relación entre dos variables, pero no representa relaciones triádicas. En este contexto, para datos multivariados, se plantea el problema de la reducción de la dimensionalidad. 

El ACP es un procedimiento matemático por el cual se transforma un conjunto de variables correlacionadas en un conjunto de variables no-correlacionadas de menor dimensión. El procedimiento se basa en la *combinación lineal* de las variables originales sujeto a la restricción de que la combinación conseve la *mayor variabilidad* posible del conjunto original de observaciones. Objetivo de la combinación lineal: maximizar la varianza (o minimiza la pérdida de información de los datos), permitiendo reducir la dimensionalidad del problema con la menor pérdida de información.

*Utilidad del ACP:*    

- objetivo *reducir dimensión*, descartar información redundante,    
- se explora existencia de *variables latentes*:  Para entender una variable latente de forma conceptual, pongamos un ejemplo: la salud general de un individuo. Este estado podría considerarse como una variable latente, sin embargo, no existe una única medida que determine este estado (el cual es un concepto abstracto), sino que es una combinación de medidas físicas (temperatura, presión arterial, azúcar en sangre, nivel de colesterol, etc.). Es este conjunto el que determina el resultado del nivel de salud.   las variables latentes capturan un fenómeno subyacente del sistema que estamos investigando, y podemos utilizar estas variables en lugar de las originales debido a su correlación

**Variablidad explicada**   
 
La variabilidad de la primer componente es máxima cuando el vector de cargas es el autovector asociado al mayor autovalor de la matriz de covarianzas. Así, para maximizar la varianza de la primera componente los coeficientes de la combinación lineal deben ser los elementos del autovector de norma 1 asociado al mayor autovalor de la matriz. 

Para buscar la segunda y sucesivas componentes se busca una nueva combinación lineal de las variables originales, que maximice la variabilidad (comprobamos que eso se logra cuando los coeficientes corresponden al autovector asociado al segundo autovalor) y sea ortogonal a la primera componente. 

Dada la formula para deducir las componentes se advierte que las mismas no pueden tener correlación.    

Los eigenvalores miden la cantidad de variabilidad retenida por cada componente principal (siendo mayores para la primera componente principal que para el resto), por lo que pueden usarse para determinar el número de componentes principales a retener.

Un eigenvalor > 1 indica que la componente principal explica más varianza de lo que lo hace una de las variables originales, estando los datos estandarizados.


# Cantidad de componentes a utilizar en un análisis CP

1. Porcentaje de variabilidad explicada: se define un % mínimo que se desea explicar y se eligen n variables hasta alcanzar ese valor.    
2. Criterio de Kaiser: consiste en retener las primeras m componentes cuyos autovalores resulten => 1, aunque algunos autores recomiendas 0.7
3. Criterio del bastón roto: a partir de un gráfico de sedimentación elegir m componentes hasta observar que la variabilidad explicada se estabiliza. Donde la pendiente se suaviza, el gradiente es menor que el segmento anterior.
4. Prueba de Esfericidad: 


# Estimación de CP a partir de datos poblacionales

Hemos desarrollado la idea de CP sobre la base de la matriz de covarianza poblaciónal. Sin embargo, es matriz difícilmente se conozca, por eso trabajamos con la matriz muestral.

# Loadings 

Si la carga (coeficiente o loading) de una de las variables en la componente principal es positiva, significa que la variable y la componente tienen una correlación positiva. En este caso, el coseno del ángulo formado por la componente y la variable es positivo. Si la carga es positiva, un individuo que tenga una puntuación alta en esa variable tendrá valores más altos en esa componente que otro individuo que tiene un menor valor en esa variable y valores similares al primero en las restantes variables.


