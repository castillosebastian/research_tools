---
title: "Analisis_componentes_Ppales"
author: "Área de Planificación Gestión y Estadística"
date: "21/3/2022"
output: html_document
params:
  nombre_dataset:
    label: "Seleccione nombre data set:"
    value: "ninguno"
    input: select
    choices: [practica1, nadadores,]
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F)
source("Unidad0_repos_and_tools.R")
# Sources tools
# https://rpubs.com/Cristina_Gil/PCA
```

# Analisis componente principal

# Importar a mano df

```{r}
# Importar a mano pegando portapales
# df = as.integer(strsplit(as.character(c("3 1 1 1 3 1 1 1 5")), " ")[[1]])
# df =  matrix(df, nrow = 3, byrow = T)
# df

```

# Importar definiendo df

```{r}
# df <- matrix(c(1,2,3, 11,12,13), nrow = 2, ncol = 3, byrow = TRUE,
#                dimnames = list(c("row1", "row2"),
#                                c("C.1", "C.2", "C.3")))
# df
```
# Importar leyendo

```{r}
df_raw <- readxl::read_excel("nadadores.xlsx")
df = df_raw %>% dplyr::select(-Nadador)  # quito columna nombre individuos
df

```

# Exploracion de datos

```{r}
kable(head(df))
```

```{r}
skim(df)
```

# Analisis PCA

# PCA con matriz de covarianza

```{r}
matriz_cov <- cov(df)
```

La función eigen() calcula ambos y los almacena en una lista bajo el nombre de vectors y values. Los eigenvalues se devuelven en orden decreciente y los eigenvectors (estandarizados) se ordenan de izquierda a derecha acorde a sus eigenvalues asociados.

```{r}
eigen(matriz_cov)
```
# Resultados con PRCOMP

Aplicamos funciòn a nuestro df, 

```{r}
pca =  prcomp(df, center = TRUE, scale. = T)
```

Los elementos center y scale almacenados en el objeto pca contienen la media y desviación típica de las variables originales, previa estandarización (en la escala original).

```{r}
pca$center
```

```{r}
pca$scale
```


La matriz “rotation” proporciona los loadings de los componentes principales (cada columna contiene el vector de loadings de cada componente principal). La función los denomina matriz de rotación ya que si multiplicáramos la matriz de datos por *$rotation*, obtendríamos las coordenadas de los datos en el nuevo sistema rotado de coordenadas. Estas coordenadas se corresponden con los scores de los componentes principales.    

En *rotation* contiene el valor de los loadings ϕ para cada componente (vector propio, autovector o eigenvector). El número máximo de componentes principales se corresponde con el mínimo(n-1,p),

```{r}
pca$rotation
```

Cuando existe una alta correlación positiva entre todas las variables, el primer componente principal tiene todas sus coordenadas del mismo signo y puede interpretarse como un promedio ponderado de todas las variables, se interpreta como factor global de 'Tamaño'. Los restantes componentes se interpretan como factores ”de forma” y típicamente tienen coordenadas positivas y negativas, que implica que contraponen unos grupos de variables frente a otros. 

Analizar en detalle el vector de *loadings* que forma cada componente ayuda a determinar què informaciòn aporta cada una. Por ejemplo la primera componente es el resultado de la siguiente combinación lineal de las variables, en este caso los tramos de la carrera.

La funciòn tambièn calcula automáticamente el valor de las componentes principales para cada observaciòn o individuo del data set (principal componente score o simplemente scores) mediante la multiplicación de los datos con los loadings como sigue:  se tienen que multiplicar los eigenvectors transpuestos por los datos originales centrados y también transpuestos.

```{r}
pca$x
```
# Varianza explicada por las CP

La varianza explicada por cada componente principal (correspondiente a los eigenvalores) la obtenemos elevando al cuadrado la desviación estándar:  

-desviación estándar de cada componente principal:

```{r}
pca$sdev
```
- Varianza

```{r}
pca$sdev^2
```

O simplemente:

```{r}
summary(pca)
```
Tener presente que los autovalores de esta matriz corresponden a la varianza de la componente y por lo tanto debe elevarse al cuadrado el desvío estandar (fila 1)


# Graficos

```{r}
biplot(x = pca, scale = 0, cex = 0.6, col = c("blue4", "brown3"))
```
Interpretacion: Para los vectores (variables), nos fijamos en su longitud y en el ángulo con respecto a los ejes de las componentes principales y entre ellos mismos:

Ángulo: cuanto más paralelo es un vector al eje de una componente, más ha contribuido a la creación de la misma. Con ello obtienes información sobre qué variable(s) ha sido más determinante para crear cada componente, y si entre las variables (y cuales) hay correlaciones. Ángulos pequeños entre vectores representa alta correlación entre las variables implicadas (observaciones con valores altos en una de esas variables tendrá valores altos en la variable o variables correlacionadas); ángulos rectos representan falta de correlación, y ángulos opuestos representan correlación negativa (una observación con valores altos en una de las variables irá acompañado de valores bajos en la otra).

Longitud: cuanto mayor la longitud de un vector relacionado con x variable (en un rango normalizado de 0 a 1), mayor variabilidad de dicha variable está contenida en la representación de las dos componentes del biplot, es decir, mejor está representada su información en el gráfico.

Para los scores (observaciones), nos fijamos en los posibles agrupamientos. Puntuaciones próximas representan observaciones de similares características. Puntuaciones con valores de las variables próximas a la media se sitúan más cerca del centro del biplot (0, 0). El resto representan variabilidades normales o extremas (outliers). 

# Otros gràficos

```{r}
prop_varianza = pca$sdev^2 / sum(pca$sdev^2)

ggplot(data = data.frame(prop_varianza, pc = 1:length(prop_varianza)),
       aes(x = pc, y = prop_varianza)) +
  geom_col(width = 0.3) +
  geom_text(label = round(prop_varianza * 100, digits = 1)) +
  scale_y_continuous(limits = c(0,1)) +
  theme_bw() +
  labs(x = "Componente principal",
       y = "Prop. de varianza explicada")

```


```{r}
prop_varianza_acum <- cumsum(prop_varianza)

ggplot(data = data.frame(prop_varianza_acum, pc = 1:length(prop_varianza_acum)),
       aes(x = pc, y = prop_varianza_acum, group = 1)) +
  geom_point() +
  geom_line() +
  geom_text(label = round(prop_varianza_acum * 100, digits = 1), vjust =-.5) +
  labs(x = "Componente principal",
       y = "Prop. varianza explicada acumulada")

```



# FactomineR
```{r}
pca2.nci <- PCA(X = df, scale.unit = TRUE, ncp = 64, graph = FALSE)
```

```{r}
print(pca2.nci)
```
```{r}
head(pca2.nci$eig)
```

 Como es de esperar, el eigenvalor (varianza explicada) es mayor en la primera componente que en las subsiguientes.
 
# Representacion 

Cabe destacar que la representación gráfica de las observaciones y las variables es distinta: las observaciones se representan mediante sus proyecciones, mientras que las variables se representan mediante sus correlaciones. La correlación entre una componente y una variable estima la información que comparten -> loadings, por lo que las variables se pueden representar como puntos en el espacio de los componentes utilizando sus loadings como coordenadas.

Observaciones
Se muestran dos ejemplos para representar las observaciones sobre las dos primeras componentes principales:

 
```{r}
fviz_pca_ind(pca2.nci, geom.ind = "point", 
             col.ind = "#FC4E07", 
             axes = c(1, 2), 
             pointsize = 1.5) 
```
 
 # axes 1 y 2 se corresponden con PC1 y PC2, pudiendo escoger otros
 
# Representacion de Variables
Para representar las variables sobre las dos primeras componentes principales podemos utilizar la función fviz_pca_var() del paquete factoextra. La correlación entre una variable y una componente principal se utiliza como la coordenada de dicha variable sobre la componente principal. De esta manera podemos obtener un gráfico de correlación de variables:

```{r}
fviz_pca_var(pca2.nci, col.var = "cos2", 
             geom.var = "arrow", 
             labelsize = 2, 
             repel = FALSE)
```

# Biplot

```{r}
biplot(pca, scale = 0, cex = 0.5, col = c("dodgerblue3", "deeppink3"))
```
 
 